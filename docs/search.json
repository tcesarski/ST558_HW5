[
  {
    "objectID": "ST558_HW5.html",
    "href": "ST558_HW5.html",
    "title": "ST 558 Homework 5",
    "section": "",
    "text": "The purpose of using cross validation when fitting a random forest model is to find mtry - the tuning parameter. This represents the random subset of predictors that is used in the model.\n\n\n\nThe bagged tree algorithm represents bootstrap aggregation. The idea is that you get a bootstrap sample, train tree on this sample, and then resample with replacement and repeat the process B number of times. Then your final prediction is an average of these predictions if using a regression tree or use a majority vote (among other options) for classification trees. This method is more ideal than a single tree because it decreases the variability in the predictions.\n\n\n\nA general linear model is a model that has a continuous response variable and allows for both continuous and categorical predictors. This could be a simple linear regression, multiple linear regression, among others. But the response and errors follow a normal distribution, which makes it different from a generalized linear model where those can come from non-normal distributions.\n\n\n\nAdding an interaction term allows you to add a term that accounts for the relationship between two predictor variables through using the notation x1:x2. Then, you can fit a “best saddle” through the points, rather than simply a best plane. This enables you to have a more flexible surface to fit to the data and make predictions.\n\n\n\nWe split our data into a training and a test set so that we can train/fit the model on part of the data and then test how well the model is performing on the other part of the data. By doing this, we ensure that we are not overfitting the model to the data we have and that the model can generalize to data it hasn’t yet seen."
  },
  {
    "objectID": "ST558_HW5.html#logistic-regression-model-1",
    "href": "ST558_HW5.html#logistic-regression-model-1",
    "title": "ST 558 Homework 5",
    "section": "Logistic Regression Model #1",
    "text": "Logistic Regression Model #1\nThe first function will use Sex, ChestPainType, FastingBS, and Exercise Angina as main effect terms, as I found these relevant during my EDA.\n\n#Not using dummy variables so using createDataPartition to select 70% of rows on the original data set.\nset.seed(100)\ntrainIndex &lt;- createDataPartition(heart_data$HeartDisease, p =0.7, list = FALSE)\n#Using selected rows on heartTrain.\nheartTrain &lt;- heart_data[trainIndex, ]\n#Using remaining rows on heartTest.\nheartTest &lt;- heart_data[-trainIndex, ]\n\n#Create the train Control using the trainControl function to do repeated cross validation.\nset.seed(50)\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n#Convert HeartDisease and FastingBS to factor.\nheartTrain$HeartDisease &lt;- as.factor(heartTrain$HeartDisease)\nheartTest$HeartDisease &lt;- as.factor(heartTest$HeartDisease)\n\n#Fit first logistic regression model using only main interaction terms.\n#Use the heartTrain dataset.\n#Use method = glm to indicate a generalized linear model.\n#Use family = binomial to indicate logistic regression.\n#Preprocess the data and use the same 10 fold repeated cross validation as above.\nlog_fit_1 &lt;- train(HeartDisease ~ ChestPainType + ExerciseAngina + FastingBS + Sex,\n                   data = heartTrain,\n                   method = \"glm\",\n                   family = \"binomial\",\n                   preProcess = c(\"center\", \"scale\"),\n                   trControl = trctrl)\n#79.7% accuracy on training set.\nlog_fit_1\n\nGeneralized Linear Model \n\n643 samples\n  4 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (6), scaled (6) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 579, 579, 578, 578, 580, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7974648  0.5858969"
  },
  {
    "objectID": "ST558_HW5.html#logistic-model-2",
    "href": "ST558_HW5.html#logistic-model-2",
    "title": "ST 558 Homework 5",
    "section": "Logistic Model #2",
    "text": "Logistic Model #2\n\n#Create a logistic model using interaction of age and sex (and main effects), as well as ChestPainType. Based on EDA, it appeared that all variables had some effect on Heart Disease, although some were stronger predictors than others.\n#Same dataset, method, trcontrol, family, etc. as defined above.\n\n#Create the train Control using the trainControl function to do repeated cross validation.\nset.seed(50)\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nlog_fit_2 &lt;- train(HeartDisease ~ Age*Sex + ChestPainType,\n                   data = heartTrain,\n                   method = \"glm\",\n                   family = \"binomial\",\n                   preProcess = c(\"center\", \"scale\"),\n                   trControl = trctrl)\n#76.8% accuracy on training set.\nlog_fit_2\n\nGeneralized Linear Model \n\n643 samples\n  3 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (6), scaled (6) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 579, 579, 578, 578, 580, ... \nResampling results:\n\n  Accuracy  Kappa    \n  0.767953  0.5286534"
  },
  {
    "objectID": "ST558_HW5.html#logistic-model-3",
    "href": "ST558_HW5.html#logistic-model-3",
    "title": "ST 558 Homework 5",
    "section": "Logistic Model #3",
    "text": "Logistic Model #3\n\n#Create a logistic model using all possible combinations of ChestPainType and Exercise Angina (since both are related to chest pain), along with Age, Sex, and FastingBS variables.\n#Same method, family, etc. as defined above.\n\n#Create the train Control using the trainControl function to do repeated cross validation.\nset.seed(50)\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nlog_fit_3 &lt;- train(HeartDisease ~ ChestPainType*ExerciseAngina + FastingBS + Age + Sex,\n                   data = heartTrain,\n                   method = \"glm\",\n                   family = \"binomial\",\n                   preProcess = c(\"center\", \"scale\"),\n                   trControl = trctrl)\n#79.4% accuracy on training set.\nlog_fit_3\n\nGeneralized Linear Model \n\n643 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (10), scaled (10) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 579, 579, 578, 578, 580, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.7938104  0.581491"
  },
  {
    "objectID": "ST558_HW5.html#pick-best-logistic-model.",
    "href": "ST558_HW5.html#pick-best-logistic-model.",
    "title": "ST 558 Homework 5",
    "section": "Pick best logistic model.",
    "text": "Pick best logistic model.\nBased on the accuracy metric, it appears that the logistic model #1 (using Sex, ChestPainType, FastingBS, and Exercise Angina as main effect terms) has the highest accuracy on the test set at 79.7%. Here is the confusionMatrix for log_fit_1 and a summary of it.\n\n#Basic summary of model.\nsummary(log_fit_1)\n\n\nCall:\nNULL\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.35920    0.11093   3.238   0.0012 ** \nChestPainTypeATA -0.82727    0.12417  -6.662 2.69e-11 ***\nChestPainTypeNAP -0.64097    0.10653  -6.017 1.78e-09 ***\nChestPainTypeTA  -0.23814    0.09457  -2.518   0.0118 *  \nExerciseAnginaY   0.93425    0.11646   8.022 1.04e-15 ***\nFastingBS1        0.57607    0.11323   5.088 3.62e-07 ***\nSexM              0.52026    0.11121   4.678 2.89e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.97  on 642  degrees of freedom\nResidual deviance: 557.41  on 636  degrees of freedom\nAIC: 571.41\n\nNumber of Fisher Scoring iterations: 5\n\n#Use confusion matrix on chosen model.\n#This model has a 80% accuracy on test set.\nconfusionMatrix(data = heartTest$HeartDisease, reference = predict(log_fit_1, newdata=heartTest))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  84  39\n         1  16 136\n                                          \n               Accuracy : 0.8             \n                 95% CI : (0.7478, 0.8456)\n    No Information Rate : 0.6364          \n    P-Value [Acc &gt; NIR] : 2.734e-09       \n                                          \n                  Kappa : 0.5882          \n                                          \n Mcnemar's Test P-Value : 0.003012        \n                                          \n            Sensitivity : 0.8400          \n            Specificity : 0.7771          \n         Pos Pred Value : 0.6829          \n         Neg Pred Value : 0.8947          \n             Prevalence : 0.3636          \n         Detection Rate : 0.3055          \n   Detection Prevalence : 0.4473          \n      Balanced Accuracy : 0.8086          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "ST558_HW5.html#classification-tree-model",
    "href": "ST558_HW5.html#classification-tree-model",
    "title": "ST 558 Homework 5",
    "section": "Classification Tree Model",
    "text": "Classification Tree Model\nFirst I will make a classification tree. The goal of a classification tree is to predict group memebership - heart disease in this case!\n\n#Set seed for reproducibility.\nset.seed(50)\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n#Create the tuneGrid by making a dataframe of the cp parameter that starts at 0, goes to 0.1 and counts by 0.001.\ntune_parameter &lt;- data.frame(cp = seq(0, 0.1, by = 0.001))\n\n#Convert the rest of the cateogorical variables to factors.\nheartTrain$Sex &lt;- as.factor(heartTrain$Sex)\nheartTrain$ChestPainType &lt;- as.factor(heartTrain$ChestPainType)\nheartTrain$RestingECG &lt;- as.factor(heartTrain$RestingECG)\nheartTrain$ExerciseAngina &lt;- as.factor(heartTrain$ExerciseAngina)\n\n\n#Create a classification tree using predictors of Exercise Angina, Chest Pain Type, Sex, MaxHR, and Fasting Blood Sugar.\n#Use the heartTrain data set.\n#Use rpart for the method.\n#Use the train control defined above (repeated 10 fold cross validation) and the tuneGrid defined above.\nclass_tree &lt;- train(HeartDisease ~ ChestPainType + FastingBS + ExerciseAngina + Sex + MaxHR,\n                    data = heartTrain,\n                    method = \"rpart\",\n                    trControl = trctrl,\n                    tuneGrid = tune_parameter)\n#Accuracy of 78.4% on training set.\nclass_tree\n\nCART \n\n643 samples\n  5 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 579, 579, 578, 578, 580, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7771913  0.5471617\n  0.001  0.7787298  0.5504858\n  0.002  0.7834180  0.5601482\n  0.003  0.7818636  0.5573315\n  0.004  0.7818793  0.5573217\n  0.005  0.7818793  0.5573217\n  0.006  0.7818305  0.5574219\n  0.007  0.7818305  0.5574219\n  0.008  0.7838492  0.5625669\n  0.009  0.7823107  0.5596089\n  0.010  0.7782162  0.5504396\n  0.011  0.7750827  0.5446403\n  0.012  0.7771334  0.5499487\n  0.013  0.7745453  0.5457652\n  0.014  0.7760515  0.5498173\n  0.015  0.7807229  0.5599807\n  0.016  0.7812358  0.5605316\n  0.017  0.7812358  0.5605316\n  0.018  0.7812358  0.5605316\n  0.019  0.7812358  0.5605316\n  0.020  0.7838319  0.5653568\n  0.021  0.7838319  0.5653568\n  0.022  0.7838319  0.5653568\n  0.023  0.7838319  0.5653568\n  0.024  0.7838319  0.5653568\n  0.025  0.7838319  0.5653568\n  0.026  0.7838319  0.5653568\n  0.027  0.7838319  0.5653568\n  0.028  0.7838319  0.5653568\n  0.029  0.7838319  0.5653568\n  0.030  0.7838319  0.5653568\n  0.031  0.7838319  0.5653568\n  0.032  0.7838319  0.5653568\n  0.033  0.7838319  0.5653568\n  0.034  0.7838319  0.5653568\n  0.035  0.7838319  0.5653568\n  0.036  0.7838319  0.5653568\n  0.037  0.7838319  0.5653568\n  0.038  0.7838319  0.5653568\n  0.039  0.7838319  0.5653568\n  0.040  0.7838319  0.5653568\n  0.041  0.7838319  0.5653568\n  0.042  0.7838319  0.5653568\n  0.043  0.7838319  0.5653568\n  0.044  0.7838319  0.5653568\n  0.045  0.7802422  0.5586292\n  0.046  0.7802422  0.5586292\n  0.047  0.7802422  0.5586292\n  0.048  0.7802422  0.5586292\n  0.049  0.7802422  0.5586292\n  0.050  0.7802422  0.5586292\n  0.051  0.7802422  0.5586292\n  0.052  0.7802422  0.5586292\n  0.053  0.7802422  0.5586292\n  0.054  0.7802422  0.5586292\n  0.055  0.7802422  0.5586292\n  0.056  0.7802422  0.5586292\n  0.057  0.7802422  0.5586292\n  0.058  0.7802422  0.5586292\n  0.059  0.7802422  0.5586292\n  0.060  0.7802422  0.5586292\n  0.061  0.7802422  0.5586292\n  0.062  0.7802422  0.5586292\n  0.063  0.7802422  0.5586292\n  0.064  0.7802422  0.5586292\n  0.065  0.7802422  0.5586292\n  0.066  0.7802422  0.5586292\n  0.067  0.7802422  0.5586292\n  0.068  0.7802422  0.5586292\n  0.069  0.7802422  0.5586292\n  0.070  0.7802422  0.5586292\n  0.071  0.7802422  0.5586292\n  0.072  0.7802422  0.5586292\n  0.073  0.7802422  0.5586292\n  0.074  0.7802422  0.5586292\n  0.075  0.7802422  0.5586292\n  0.076  0.7802422  0.5586292\n  0.077  0.7802422  0.5586292\n  0.078  0.7802422  0.5586292\n  0.079  0.7802422  0.5586292\n  0.080  0.7802422  0.5586292\n  0.081  0.7802422  0.5586292\n  0.082  0.7678864  0.5360531\n  0.083  0.7678864  0.5360531\n  0.084  0.7678864  0.5360531\n  0.085  0.7678864  0.5360531\n  0.086  0.7626781  0.5262721\n  0.087  0.7626781  0.5262721\n  0.088  0.7626781  0.5262721\n  0.089  0.7626781  0.5262721\n  0.090  0.7523575  0.5084414\n  0.091  0.7523575  0.5084414\n  0.092  0.7523575  0.5084414\n  0.093  0.7492325  0.5032898\n  0.094  0.7476941  0.5005874\n  0.095  0.7476941  0.5005874\n  0.096  0.7476941  0.5005874\n  0.097  0.7435269  0.4938369\n  0.098  0.7435269  0.4938369\n  0.099  0.7435269  0.4938369\n  0.100  0.7435269  0.4938369\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.008."
  },
  {
    "objectID": "ST558_HW5.html#random-forest-model",
    "href": "ST558_HW5.html#random-forest-model",
    "title": "ST 558 Homework 5",
    "section": "Random Forest Model",
    "text": "Random Forest Model\nIn this model we will use a random forest meaning that we will choose a random subset of predictors to create the model. We will get the random subset of predictors using cross validation. I will use the same predictors as the classification tree (Exercise Angina, Chest Pain Type, Sex, MaxHR, and FastingBS) to make for easier comparison.\n\n#Use the heartTrain data set.\n#Use rf for the method.\n#Use the train control defined above (repeated 10 fold cross validation) and the tuneGrid is from 1 to 5 because I have chosen five predictors.\n#Set seed for reproducibility and do 3 repeats of 10 fold cross validation.\nset.seed(50)\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n\nrf_model &lt;- train(HeartDisease ~ ChestPainType + FastingBS + ExerciseAngina + Sex + MaxHR,\n                    data = heartTrain,\n                    method = \"rf\",\n                    trControl = trctrl,\n                    tuneGrid = data.frame(mtry = 1:5))\n#Accuracy of 81.5% on training set.\nrf_model\n\nRandom Forest \n\n643 samples\n  5 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 579, 579, 578, 578, 580, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  1     0.8124180  0.6204563\n  2     0.8150622  0.6282650\n  3     0.8015185  0.5994622\n  4     0.7963988  0.5883370\n  5     0.7808689  0.5564287\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2."
  },
  {
    "objectID": "ST558_HW5.html#boosted-tree",
    "href": "ST558_HW5.html#boosted-tree",
    "title": "ST 558 Homework 5",
    "section": "Boosted Tree",
    "text": "Boosted Tree\nHere I will create a boosted tree model to grow the tree sequentially. I will use the same predictors as the classification tree (Exercise Angina, Chest Pain Type, Sex, MaxHR, and FastingBS) to make for easier comparison.\n\n#Set seed for reproducibility and do 3 repeats of 10 fold cross validation.\nset.seed(50)\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n\n\n#Create vectors of tuning parameters.\ntune1 &lt;- c(25,50,100,200)\ntune2 &lt;- 1:3\ntune3 &lt;- 0.1\ntune4 &lt;- 10\n\n\n#Use expand.grid to create all possible combinations of the parameters.\ntune_parameters &lt;- expand.grid(n.trees = tune1, \n            interaction.depth = tune2,\n            shrinkage = tune3,\n            n.minobsinnode = tune4)\n#Print out data frame of tuning parameters.\ntune_parameters\n\n   n.trees interaction.depth shrinkage n.minobsinnode\n1       25                 1       0.1             10\n2       50                 1       0.1             10\n3      100                 1       0.1             10\n4      200                 1       0.1             10\n5       25                 2       0.1             10\n6       50                 2       0.1             10\n7      100                 2       0.1             10\n8      200                 2       0.1             10\n9       25                 3       0.1             10\n10      50                 3       0.1             10\n11     100                 3       0.1             10\n12     200                 3       0.1             10\n\n#Use the heartTrain data set.\n#Use gbm for the method.\n#Use the train control defined above (repeated 10 fold cross validation) and the tuneGrid defined above.\n\nboost_tree &lt;- train(HeartDisease ~ ChestPainType + Sex + Age + RestingECG + MaxHR + ExerciseAngina,\n                    data = heartTrain,\n                    method = \"gbm\",\n                    trControl = trctrl,\n                    tuneGrid = tune_parameters,\n                    verbose = FALSE)\n\n#Accuracy of 79.3% on the training set.\nboost_tree\n\nStochastic Gradient Boosting \n\n643 samples\n  6 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 579, 579, 578, 578, 580, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.7776320  0.5506112\n  1                   50      0.7803093  0.5549724\n  1                  100      0.7871130  0.5687645\n  1                  200      0.7917519  0.5783497\n  2                   25      0.7808873  0.5555178\n  2                   50      0.7865366  0.5675821\n  2                  100      0.7896939  0.5748306\n  2                  200      0.7870884  0.5679352\n  3                   25      0.7772898  0.5474648\n  3                   50      0.7824733  0.5581344\n  3                  100      0.7934121  0.5805618\n  3                  200      0.7819607  0.5574094\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 100, interaction.depth =\n 3, shrinkage = 0.1 and n.minobsinnode = 10."
  },
  {
    "objectID": "ST558_HW5.html#choose-best-tree-model",
    "href": "ST558_HW5.html#choose-best-tree-model",
    "title": "ST 558 Homework 5",
    "section": "Choose best tree model",
    "text": "Choose best tree model\nIt appears that the random forest model gives the best accuracy on the training data set. Here is a confusion matrix for the test set to see the accuracy there.\n\nconfusionMatrix(data = heartTest$HeartDisease, predict(rf_model, newdata=heartTest))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  96  27\n         1  30 122\n                                        \n               Accuracy : 0.7927        \n                 95% CI : (0.74, 0.8391)\n    No Information Rate : 0.5418        \n    P-Value [Acc &gt; NIR] : &lt;2e-16        \n                                        \n                  Kappa : 0.5818        \n                                        \n Mcnemar's Test P-Value : 0.7911        \n                                        \n            Sensitivity : 0.7619        \n            Specificity : 0.8188        \n         Pos Pred Value : 0.7805        \n         Neg Pred Value : 0.8026        \n             Prevalence : 0.4582        \n         Detection Rate : 0.3491        \n   Detection Prevalence : 0.4473        \n      Balanced Accuracy : 0.7903        \n                                        \n       'Positive' Class : 0             \n                                        \n\n#79.27% accuracy on the testing data set."
  }
]