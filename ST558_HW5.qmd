---
title: "ST 558 Homework 5"
format: html
author: "Taylor Cesarski"
---

# Task 1: Conceptual Questions
### Question 1: What is the purpose of using cross-validation when fitting a random forest model?

The purpose of cross validation in general is to split data multiple ways, do the fitting/testing process, and then combine results. Sometimes when using a training or test set you may not have enough data or you may get a weird split of the data so it can be helpful to split into many folds where you train on the first k-1 folds and test on the kth fold. Then repeat the process. In the context of a random forest model (and bagged tree models in general), this process happens by using out of bag observations. With repeated bootstrap samples, they only use about 2/3 of the data in the full sample so then you have a remaining portion to test on that wasn't used in the training of the model. 


### Question 2: Describe the bagged tree algorithm.

The bagged tree algorithm represents bootstrap aggregation. The idea is that you get a bootstrap sample, train tree on this sample, and then resample with replacement and repeat the process B number of times. Then your final prediction is an average of these predictions if using a regression tree or use a majority vote (among other options) for classification trees. This method is more ideal than a single tree because it decreases the variability in the predictions. 


### Question 3: What is meant by a general linear model?

A general linear model is a model that has a continuous response variable and allows for both continuous and categorical predictors. This could be a simple linear regression, multiple linear regression, among others.


### Question 4: When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

Adding an interaction term allows you to add a term that accounts for the relationship between two predictor variables through using the notation x1:x2. Then, you can fit a "best saddle" through the points, rather than simply a best plane. This enables you to have a more flexible surface to fit to the data and make predictions.


### Question 5: Why do we split our data into a training and test set?

We split our data into a training and a test set so that we can train/fit the model on part of the data and then test how well the model is performing on the other part of the data. By doing this, we ensure that we are not overfitting the model to the data we have and that the model can generalize to data it hasn't yet seen. 


```{r, warning = FALSE, message = FALSE}
library(tidyverse)
#Read in the heart dataset.
heart_data <- read_csv("heart.csv")
```

# Quick EDA/Data Preparation - Questions 1 and 2.
Based on the EDA that follows, it appears that the following predictors have the strongest relationship to heart disease:
* Categorical: ChestPainType, Sex, RestingECG, FastingBS, ExerciseAngina
* Numeric: MaxHR, Cholesterol, Age
```{r}
#Write over the heart dataset. Remove the ST_Slope column and convert the HeartDisease and FastingBS columns to factors.
heart_data <- heart_data |>
  select(-ST_Slope) |>
  mutate(HeartDisease = as.factor(HeartDisease),
         FastingBS = as.factor(FastingBS))
#Print the first few rows of the dataset to check that it is working correctly.
head(heart_data)

#Check for missing values. It appears there are no missing values.
missing_vals <- colSums(is.na(heart_data))
missing_vals

#Make a stacked bar chart of heart disease by gender.
#It appears males get heart disease more frequently than females.
ggplot(heart_data, aes(x = Sex, fill = HeartDisease)) +
  geom_bar()

#Make a stacked bar chart of heart disease by RestingECG
#It appears those with a RestingECG of ST or LVH are more likely to have heart disease.
ggplot(heart_data, aes(x = RestingECG, fill = HeartDisease)) +
  geom_bar()

#Make a stacked bar chart of heart disease by fasting blood sugar.
#Info from data source: FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]
#It appears those with a fasting blood sugar above 120 have a higher likelihood of heart disease.
ggplot(heart_data, aes(x = FastingBS, fill = HeartDisease)) +
  geom_bar()

#Make a stacked bar chart of heart disease by Exercise Angina.
#It appears those with Exercise Angina are more likely to have heart disease.
ggplot(heart_data, aes(x = ExerciseAngina, fill = HeartDisease)) +
  geom_bar()

#Density Plot of Oldpeak faceted by Heart Disease.
#Looks like maybe higher. Not sure if significant. Investigate numerically down below.
ggplot(heart_data, aes(x = Oldpeak))+
  geom_density() +
  facet_wrap(~HeartDisease)


#Density Plot of Resting BP faceted by Heart Disease.
#It appears that resting BP is slightly higher in those with heart disease.
ggplot(heart_data, aes(x = RestingBP))+
  geom_density() +
  facet_wrap(~HeartDisease)

#Boxplots of age by heart disease.
#It appears that as age increases, likelihood of heart disease increases as well.
ggplot(heart_data, aes(x = Age, y = HeartDisease)) +
  geom_boxplot()

#Boxplots of cholesterol by heart disease.
#Surprisingly, cholesterol appears lower with heart disease, but there is a lot of variability in the non-heart disease group.
ggplot(heart_data, aes(x = Cholesterol, y = HeartDisease)) +
  geom_boxplot()

#Chest pain type and heart disease bar plots.
#It appears that ASY chest pain is a strong indicator of heart disease.
ggplot(heart_data, aes(x = ChestPainType, fill = HeartDisease)) +
  geom_bar()


#Means of HR, Age, BP, and chol for those with and without heart disease. 
#Group with heart disease has lower max HR, higher age, slightly higher BP, slightly lower Oldpeak, and lower cholesterol. 
num_sums <- heart_data |>
  group_by(HeartDisease) |>
  summarize(mean_max_hr = mean(MaxHR),
            mean_age = mean(Age),
            mean_bp = mean(RestingBP),
            mean_chol = mean(Cholesterol),
            mean_oldpeak = mean(Oldpeak))

num_sums

```


# Creating Dummy Variables - EDA Section #3
I am taking the approach of defining the numeric predictors in one dataset, the target response (HeartDisease) in another, and the dummy variables in another. Then I will use cbind to combine the columns.
```{r, warning = FALSE}
#Read in the caret package.
library(caret)

#Save the numeric precitors as num_heart.
num_heart <- heart_data |>
  select(Age, RestingBP, Cholesterol, MaxHR, Oldpeak)
#Save the target predictor (HeartDisaese) as target_heart.
target_heart <- heart_data |>
  select(HeartDisease)

#Create the dummy variables using the dummyVars function from the caret package.
dummies <- dummyVars(HeartDisease ~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_data)
#use predict to create the new columns.
dummy_data <- predict(dummies, newdata = heart_data)

#Combine all the columns using cbind and then print the first few rows of the combined dataset.
heart_final <- cbind(num_heart, dummy_data, target_heart)
head(heart_final)
```

# Split your Data
```{r}
#Set the seeda at 100 for reproducibility.
set.seed(100)
#Use the createDataPartition from the caret package. Get 70% of the rows.
trainIndex <- createDataPartition(heart_final$HeartDisease, p =0.7, list = FALSE)
#Assign those 70% to the training dataset.
subheartTrain <- heart_final[trainIndex, ]
#Assign the rows not selected in the trainIndex (30% of data) to the test dataset.
subheartTest <- heart_final[-trainIndex, ]
```

# kNN
```{r}
#Set the train control to repeated 10 fold cross validation with 3 repeats. This will be used later when training the model.
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
#Set seed for reproducibility.
set.seed(50)
#Based on EDA, I chose to use MaxHR, Age, and Cholesterol in the model. I chose to do main effect & all interaction terms.
#Use train function on the subheartTrain dataset created above.
#Want the k nearest neighbors method. 
#Center and scale using preprocess.
#Create a tune grid data frame from 1 to 40 to pick the best number of number of neighbors to use.
knn_fit <- train(HeartDisease ~ MaxHR*Age*Cholesterol,
                 data = subheartTrain, 
                 method = "knn",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneGrid = data.frame(k = 1:40))

#It appears that the best fit was using 38 neighbors as this gave the highest accuracy.
knn_fit

#Test the model on the test data set.
knn_predict <- predict(knn_fit, newdata = subheartTest)

confusionMatrix(knn_predict, subheartTest$HeartDisease)

#This correctly predicts heart disease about 72.36% of the time in the test data set.

```
# Logistic Regression
## Logistic Regression Model #1
```{r}
#Not using dummy variables so using createDataPartition to select 70% of rows on the original data set.
set.seed(100)
trainIndex <- createDataPartition(heart_data$HeartDisease, p =0.7, list = FALSE)
#Using selected rows on heartTrain.
heartTrain <- heart_data[trainIndex, ]
#Using remaining rows on heartTest.
heartTest <- heart_data[-trainIndex, ]


#Fit first logistic regression model using only main interaction terms.
#Use the heartTrain dataset.
#Use method = glm to indicate a generalized linear model.
#Use family = binomial to indicate logistic regression.
#Preprocess the data and use the same 10 fold repeated cross validation as above.
log_fit_1 <- train(HeartDisease ~ ChestPainType + ExerciseAngina + FastingBS + MaxHR + Sex,
                   data = heartTrain,
                   method = "glm",
                   family = "binomial",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
#80.39% accuracy on training set.
log_fit_1


#Use a confusion matrix to check accuracy on test set.
#This model has a 82.18% accuracy on test set.

confusionMatrix(data = heartTest$HeartDisease, reference = predict(log_fit_1, newdata=heartTest))
```

## Logistic Model #2 

```{r}
#Create a logistic model using all predictors. Based on EDA, it appeared that all variables had some effect on Heart Disease, although some were stronger predictors than others.
#Same dataset, method, trcontrol, family, etc. as defined above.
log_fit_2 <- train(HeartDisease ~ .,
                   data = heartTrain,
                   method = "glm",
                   family = "binomial",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
#80.77% accuracy on training set.
log_fit_2


#Use a confusion matrix to check accuracy on test set.
#This model has a 84% accuracy on test set.
confusionMatrix(data = heartTest$HeartDisease, reference = predict(log_fit_2, newdata=heartTest))
```

## Logistic Model #3

```{r}
#Create a logistic model using all possible combinations of some of the variables (and a main effect sex term that seemed most relevant (based on the EDA) to the prediction of Heart Disease. 
#Same method, family, etc. as defined above.
log_fit_3 <- train(HeartDisease ~ ChestPainType*ExerciseAngina + FastingBS*MaxHR + Sex,
                   data = heartTrain,
                   method = "glm",
                   family = "binomial",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
#80.35% accuracy on training set.
log_fit_3

#Use a confusion matrix to check accuracy on test set.
#This model has a 80.73% accuracy on test set.
confusionMatrix(data = heartTest$HeartDisease, reference = predict(log_fit_3, newdata=heartTest))
```



## Pick best logistic model.
Based on the accuracy metric, it appears that the logistic model #2 (using all predictors) has the highest accuracy on the test set. Here is the confusionMatrix again for log_fit_2 and a summary of it.

```{r}
#Basic summary of model.
summary(log_fit_2)

#Use confusion matrix on chosen model.
#This model has a 84% accuracy on test set.
confusionMatrix(data = heartTest$HeartDisease, reference = predict(log_fit_2, newdata=heartTest))

```

# Tree Models

```{r, message=FALSE, warning=FALSE}
#Load in required libraries.
library(tree)
library(rpart)
```


## Classification Tree Model
```{r}
#Set seed for reproducibility.
set.seed(150)
#Same code as above. Repeated 10 fold cross validation.
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

#Create the tuneGrid by making a dataframe of the cp parameter that starts at 0, goes to 0.1 and counts by 0.001.
tune_parameter <- data.frame(cp = seq(0, 0.1, by = 0.001))

#Convert to factors or else get Error: Zero-length variable name.
heartTrain$Sex <- as.factor(heartTrain$Sex)
heartTrain$ChestPainType <- as.factor(heartTrain$ChestPainType)
heartTrain$RestingECG <- as.factor(heartTrain$RestingECG)
heartTrain$ExerciseAngina <- as.factor(heartTrain$ExerciseAngina)


#Create a classification tree using main effect terms of Exercise Angina, Chest Pain Type, Sex, MaxHR, and Fasting Blood Sugar.
#Use the heartTrain data set.
#Use rpart for the method.
#Use the train control defined above (repeated 10 fold cross validation) and the tuneGrid defined above.
class_tree <- train(HeartDisease ~ ChestPainType + FastingBS + ExerciseAngina + Sex + MaxHR,
                    data = heartTrain,
                    method = "rpart",
                    trControl = trctrl,
                    tuneGrid = tune_parameter)
#Accuracy of 80.1% on training set.
class_tree

#Use confusion matrix on chosen model.
#This model has a 78.55% accuracy on test set.
confusionMatrix(data = heartTest$HeartDisease, reference = predict(class_tree, newdata=heartTest))


```

## Random Forest Model
```{r}
#Create a random forest model using same predictors as classification tree for easier comparison. (Exercise Angina, Chest Pain Type, Sex, MaxHR, and Fasting Blood Sugar)
#Use the heartTrain data set.
#Use rf for the method.
#Use the train control defined above (repeated 10 fold cross validation) and the tuneGrid defined above.
rf_model <- train(HeartDisease ~ ChestPainType + FastingBS + ExerciseAngina + Sex + MaxHR,
                    data = heartTrain,
                    method = "rf",
                    trControl = trctrl,
                    tuneGrid = data.frame(mtry = 1:5))
#Accuracy of 81.5% on training set.
rf_model

#Use confusion matrix on chosen model.
#This model has a 79.64% accuracy on test set.
confusionMatrix(data = heartTest$HeartDisease, reference = predict(rf_model, newdata=heartTest))


```


## Boosted Tree

```{r}
#Create vectors of tuning parameters.
tune1 <- c(25,50,100,200)
tune2 <- 1:3
tune3 <- 0.1
tune4 <- 10


#Use expand.grid to create all possible combinations of the parameters.
tune_parameters <- expand.grid(n.trees = tune1, 
            interaction.depth = tune2,
            shrinkage = tune3,
            n.minobsinnode = tune4)
#Print out data frame of tuning parameters.
tune_parameters

#Create a boosted tree model using same predictors as classification tree & random forest for easier comparison. (Exercise Angina, Chest Pain Type, Sex, MaxHR, and Fasting Blood Sugar)
#Use the heartTrain data set.
#Use gbm for the method.
#Use the train control defined above (repeated 10 fold cross validation) and the tuneGrid defined above.

boost_tree <- train(HeartDisease ~ ChestPainType + Sex + Age + RestingECG + MaxHR + ExerciseAngina,
                    data = heartTrain,
                    method = "gbm",
                    trControl = trctrl,
                    tuneGrid = tune_parameters,
                    verbose = FALSE)

#Accuracy of 79.27% on the training set.
boost_tree

#Use confusion matrix on chosen model.
#This model has a 78.55% accuracy on test set.
confusionMatrix(data = heartTest$HeartDisease, reference = predict(boost_tree, newdata=heartTest))
```

# Wrap Up

Based on the models, the logistic regression model using all predictors appears to be the best fit with an 84% accuracy metric on the test set.



