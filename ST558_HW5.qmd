---
title: "ST 558 Homework 5"
format: html
author: "Taylor Cesarski"
---

# Task 1: Conceptual Questions
### Question 1: What is the purpose of using cross-validation when fitting a random forest model?

The purpose of cross validation in general is to split data multiple ways, do the fitting/testing process, and then combine results. Sometimes when using a training or test set you may not have enough data or you may get a weird split of the data so it can be helpful to split into many folds where you train on the first k-1 folds and test on the kth fold. Then repeat the process. In the context of a random forest model (and bagged tree models in general), this process happens by using out of bag observations. With repeated bootstrap samples, they only use about 2/3 of the data in the full sample so then you have a remaining portion to test on that wasn't used in the training of the model. 


### Question 2: Describe the bagged tree algorithm.

The bagged tree algorithm represents bootstrap aggregation. The idea is that you get a bootstrap sample, train tree on this sample, and then resample with replacement and repeat the process B number of times. Then your final prediction is an average of these predictions if using a regression tree or use a majority vote (among other options) for classification trees. This method is more ideal than a single tree because it decreases the variability in the predictions. 


### Question 3: What is meant by a general linear model?

A general linear model is a model that has a continuous response variable and allows for both continuous and categorical predictors. This could be a simple linear regression, multiple linear regression, among others.


### Question 4: When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

Adding an interaction term allows you to add a term that accounts for the relationship between two predictor variables through using the notation x1:x2. Then, you can fit a "best saddle" through the points, rather than simply a best plane. This enables you to have a more flexible surface to fit to the data and make predictions.


### Question 5: Why do we split our data into a training and test set?

We split our data into a training and a test set so that we can train/fit the model on part of the data and then test how well the model is performing on the other part of the data. By doing this, we ensure that we are not overfitting the model to the data we have and that the model can generalize to data it hasn't yet seen. 


```{r}
library(tidyverse)
heart_data <- read_csv("heart.csv")
```

# Quick EDA/Data Preparation - Questions 1 and 2.
```{r}
heart_data <- heart_data |>
  select(-ST_Slope) |>
  mutate(HeartDisease = as.factor(HeartDisease),
         FastingBS = as.factor(FastingBS))
heart_data
#Check for missing values.

missing_vals <- colSums(is.na(heart_data))
missing_vals

#Make a stacked bar chart of heart disease by gender.
#It appears males get heart disease more frequently than females.
ggplot(heart_data, aes(x = Sex, fill = HeartDisease)) +
  geom_bar()

#Density Plot of Resting BP faceted by Heart Disease.
#It appears that resting BP is slightly higher in those with heart disease.
ggplot(heart_data, aes(x = RestingBP))+
  geom_density() +
  facet_wrap(~HeartDisease)

#Boxplots of age by heart disease.
#It appears that as age increases, likelihood of heart disease increases as well.
ggplot(heart_data, aes(x = Age, y = HeartDisease)) +
  geom_boxplot()

#Boxplots of cholesterol by heart disease.
#Surprisingly, cholesterol appears lower with heart disease, but there is a lot of variability in the non-heart disease group.
ggplot(heart_data, aes(x = Cholesterol, y = HeartDisease)) +
  geom_boxplot()

#Chest pain type and heart disease bar plots.
#It appears that ASY chest pain is a strong indicator of heart disease.
ggplot(heart_data, aes(x = ChestPainType, fill = HeartDisease)) +
  geom_bar()


#Make two way contingency table of heart disease and fasting blood sugar. 
table(heart_data$HeartDisease, heart_data$FastingBS)

#Means of HR, Age, BP, and chol for those with and without heart disease. 
#Group with heart disease has lower max HR, higher age, slightly higher BP, and lower cholesterol. 
heart_data |>
  group_by(HeartDisease) |>
  summarize(mean_max_hr = mean(MaxHR),
            mean_age = mean(Age),
            mean_bp = mean(RestingBP),
            mean_chol = mean(Cholesterol))

heart_data

```

```{r}
library(caret)
heart_data

#Numeric predcitors
num_heart <- heart_data |>
  select(Age, RestingBP, Cholesterol, MaxHR, Oldpeak)
target_heart <- heart_data |>
  select(HeartDisease)




dummies <- dummyVars(HeartDisease ~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_data)
dummies
dummy_data <- predict(dummies, newdata = heart_data)

heart_final <- cbind(num_heart, dummy_data, target_heart)
head(heart_final)
```

# Split your Data
```{r}
set.seed(100)
trainIndex <- createDataPartition(heart_final$HeartDisease, p =0.7, list = FALSE)
heartTrain <- heart_final[trainIndex, ]
heartTest <- heart_final[-trainIndex, ]
```

# kNN

```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(50)
#Based on EDA, I chose to use MaxHR, Age, and Cholesterol in the model. It didn't seem like there was a significant difference in blood pressure.
knn_fit <- train(HeartDisease ~ MaxHR + Age + Cholesterol,
                 data = heartTrain, 
                 method = "knn",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneGrid = data.frame(k = 1:40))

#It appears that the best fit was using 30 neighbors as this gave the highest accuracy.
knn_fit

#Test the model on the test data set.
knn_predict <- predict(knn_fit, newdata = heartTest)

confusionMatrix(knn_predict, heartTest$HeartDisease)

#This correctly predicts heart disease about 71.64% of the time.


```


# Logistic Regression
```{r}
#Not using dummy variables.
set.seed(100)
trainIndex <- createDataPartition(heart_data$HeartDisease, p =0.7, list = FALSE)
heartTrain <- heart_data[trainIndex, ]
heartTest <- heart_data[-trainIndex, ]

heartTrain

#Logistic with just some main interaction terms.

log_fit_1 <- train(HeartDisease ~ ChestPainType + Age + Sex,
                   data = heartTrain,
                   method = "glm",
                   family = "binomial",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
log_fit_1

#See which ones predicted correctly on test set.

confusionMatrix(data = heartTest$HeartDisease, reference = predict(log_fit_1, newdata=heartTest))

#80.36% accuracy on test set.


#Logistic model using all predictors.

log_fit_2 <- train(HeartDisease ~ .,
                   data = heartTrain,
                   method = "glm",
                   family = "binomial",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
log_fit_2


#See which ones predicted correctly on test set.

confusionMatrix(data = heartTest$HeartDisease, reference = predict(log_fit_2, newdata=heartTest))

#84% accuracy on test set.


#Logistic model wtih interaction terms.

log_fit_3 <- train(HeartDisease ~ ChestPainType*Age + Sex*Cholesterol,
                   data = heartTrain,
                   method = "glm",
                   family = "binomial",
                   preProcess = c("center", "scale"),
                   trControl = trctrl)
log_fit_3

confusionMatrix(data = heartTest$HeartDisease, reference = predict(log_fit_3, newdata=heartTest))

#81.8% accuracy on test set.




```



